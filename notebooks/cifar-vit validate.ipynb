{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ec2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c73d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations, same as the ViT training scripts if without autoaug policy\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f54c46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load train set\n",
    "train_set = dsets.CIFAR10('../data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# Load test set (using as validation)\n",
    "val_set = dsets.CIFAR10('../data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec54b9",
   "metadata": {},
   "source": [
    "# train vit model on cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb842478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os.path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10684675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2204415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f11265",
   "metadata": {},
   "source": [
    "### training of the original model (ViT but lighter) is handled by the scripts in directory ViT-CIFAR\n",
    "\n",
    "we just need to load the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a9cd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c314cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../trained_models/cifar_c10_vit_surrogate.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "569b7f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(model_path):\n",
    "    # load saved model\n",
    "    print(\"Loading saved model\")\n",
    "    model = torch.load(model_path).to(device)\n",
    "else:\n",
    "    # Create model from trained\n",
    "    print(\"Creating model, from elsewhere pretrained\")\n",
    "    model = ViT(in_c = 3, num_classes=10, img_size=32, patch=8, dropout=0.0, num_layers=7,\n",
    "           hidden=384, mlp_hidden=384, head=12, is_cls_token=True)\n",
    "    loaded = torch.load(\"../ViT-CIFAR/weights/vit_c10_aa_ls_note_oriforFastSHAP.pth\")\n",
    "    model_states = {key[6:]:loaded[key] for key in loaded}\n",
    "    model.load_state_dict(model_states)\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc36e7",
   "metadata": {},
   "source": [
    "SAME as the resnet model, ViT model outputs unnormalized logits (scores) for each class, which serve as inputs to the Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad52525",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1e43f",
   "metadata": {},
   "source": [
    "# train surrogates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ff9508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastshap import ImageSurrogate\n",
    "from fastshap.utils import MaskLayer2d, DatasetInputOnly, KLDivLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "108624d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "surr_model_path = '../trained_models/cifar_c10_vit_surrogate_lb20.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c2c6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved surrogate model\n"
     ]
    }
   ],
   "source": [
    "# Check for model\n",
    "if os.path.isfile(surr_model_path):\n",
    "    print('Loading saved surrogate model')\n",
    "    surr = torch.load(surr_model_path).to(device)\n",
    "    surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)\n",
    "else:\n",
    "    print(\"Creating new surrogate model\")\n",
    "    surr = nn.Sequential(\n",
    "        MaskLayer2d(value=0, append=True),\n",
    "        ViT(in_c = 4, num_classes=10, img_size=32, patch=8, dropout=0.0, num_layers=7,\n",
    "        hidden=384, mlp_hidden=384, head=12, is_cls_token=True)).to(device)\n",
    "    # Set up surrogate object\n",
    "    surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dc91d15-0dc1-4c37-a08b-2aed33a94016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): MaskLayer2d()\n",
       "  (1): ViT(\n",
       "    (emb): Linear(in_features=64, out_features=384, bias=True)\n",
       "    (enc): Sequential(\n",
       "      (0): TransformerEncoder(\n",
       "        (la1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (msa): MultiHeadSelfAttention(\n",
       "          (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (o): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (la2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (4): GELU()\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerEncoder(\n",
       "        (la1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (msa): MultiHeadSelfAttention(\n",
       "          (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (o): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (la2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (4): GELU()\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerEncoder(\n",
       "        (la1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (msa): MultiHeadSelfAttention(\n",
       "          (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (o): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (la2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (4): GELU()\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): TransformerEncoder(\n",
       "        (la1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (msa): MultiHeadSelfAttention(\n",
       "          (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (o): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (la2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (4): GELU()\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): TransformerEncoder(\n",
       "        (la1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (msa): MultiHeadSelfAttention(\n",
       "          (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (o): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (la2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (4): GELU()\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): TransformerEncoder(\n",
       "        (la1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (msa): MultiHeadSelfAttention(\n",
       "          (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (o): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (la2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (4): GELU()\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): TransformerEncoder(\n",
       "        (la1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (msa): MultiHeadSelfAttention(\n",
       "          (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (o): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (la2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (4): GELU()\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=384, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be070551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrain just go from here \n",
    "# print(\"Creating new surrogate model\")\n",
    "# surr = nn.Sequential(\n",
    "#     MaskLayer2d(value=0, append=True),\n",
    "#     ViT(in_c = 4, num_classes=10, img_size=32, patch=8, dropout=0.0, num_layers=7,\n",
    "#     hidden=384, mlp_hidden=384, head=12, is_cls_token=True)).to(device)\n",
    "# # Set up surrogate object\n",
    "# surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2488eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up datasets\n",
    "train_surr = DatasetInputOnly(train_set)\n",
    "val_surr = DatasetInputOnly(val_set)\n",
    "original_model = nn.Sequential(model, nn.Softmax(dim=1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d391ccc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wc/763jgpdd6m5gljcl93zb8c940000gn/T/ipykernel_38158/285035966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m surrogate.train_original_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_surr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_surr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moriginal_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/fastshap/image_surrogate.py\u001b[0m in \u001b[0;36mtrain_original_model\u001b[0;34m(self, train_data, val_data, original_model, batch_size, max_epochs, loss_fn, validation_samples, validation_batch_size, lr, min_lr, lr_factor, lookback, training_seed, validation_seed, num_workers, bar, verbose)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_seed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mS_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mvalidation_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/fastshap/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_included\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_players\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "surrogate.train_original_model(\n",
    "    train_surr,\n",
    "    val_surr,\n",
    "    original_model,\n",
    "    batch_size=256,\n",
    "    max_epochs=100,\n",
    "    loss_fn=KLDivLoss(),\n",
    "    lookback=20, #10\n",
    "    bar=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "surr.cpu()\n",
    "# torch.save(surr, 'cifar_c10_vit_surrogate_lb20.pt') # lookback 20, best val loss 0.7623\n",
    "torch.save(surr, surr_model_path) # lookback 20, best val loss, best vak kiss 0.5698\n",
    "# ls 0.827"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d77c45",
   "metadata": {},
   "source": [
    "# train fastshap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "171e3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "from fastshap import FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_path = 'cifar_explainer_unet_lb20.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model\n",
    "if os.path.isfile(explainer_path):\n",
    "    print('Loading saved explainer model')\n",
    "    explainer = torch.load(explainer_path).to(device)\n",
    "    fastshap = FastSHAP(explainer, surrogate, link=nn.LogSoftmax(dim=1))\n",
    "\n",
    "else:\n",
    "    # Set up explainer model\n",
    "    explainer = UNet(n_classes=10, num_down=2, num_up=1, num_convs=3).to(device)\n",
    "\n",
    "    # Set up FastSHAP object\n",
    "    fastshap = FastSHAP(explainer, surrogate, link=nn.LogSoftmax(dim=1))\n",
    "\n",
    "    # Set up datasets\n",
    "    fastshap_train = DatasetInputOnly(train_set)\n",
    "    fastshap_val = DatasetInputOnly(val_set)\n",
    "\n",
    "    # Train\n",
    "    fastshap.train(\n",
    "        fastshap_train,\n",
    "        fastshap_val,\n",
    "        batch_size=128,\n",
    "        num_samples=2,\n",
    "        max_epochs=200,\n",
    "        eff_lambda=1e-2,\n",
    "        validation_samples=1,\n",
    "        lookback=10,\n",
    "        bar=True,\n",
    "        verbose=True)\n",
    "    \n",
    "    # Save explainer\n",
    "    explainer.cpu()\n",
    "    torch.save(explainer, explainer_path)\n",
    "    explainer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395d911",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image from each class\n",
    "dset = val_set\n",
    "targets = np.array(dset.targets)\n",
    "num_classes = targets.max() + 1\n",
    "inds_lists = [np.where(targets == cat)[0] for cat in range(num_classes)]\n",
    "inds = [np.random.choice(cat_inds) for cat_inds in inds_lists]\n",
    "x, y = zip(*[dset[ind] for ind in inds])\n",
    "x = torch.stack(x)\n",
    "\n",
    "# Get explanations\n",
    "values = fastshap.shap_values(x.to(device))\n",
    "\n",
    "# Get predictions\n",
    "pred = surrogate(\n",
    "    x.to(device),\n",
    "    torch.ones(num_classes, surrogate.num_players, device=device)\n",
    ").softmax(dim=1).cpu().data.numpy()\n",
    "\n",
    "fig, axarr = plt.subplots(num_classes, num_classes + 1, figsize=(22, 20))\n",
    "\n",
    "for row in range(num_classes):\n",
    "    # Image\n",
    "    classes = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])[:, np.newaxis, np.newaxis]\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])[:, np.newaxis, np.newaxis]\n",
    "    im = x[row].numpy() * std + mean\n",
    "    im = im.transpose(1, 2, 0).astype(float)\n",
    "    im = np.clip(im, a_min=0, a_max=1)\n",
    "    axarr[row, 0].imshow(im, vmin=0, vmax=1)\n",
    "    axarr[row, 0].set_xticks([])\n",
    "    axarr[row, 0].set_yticks([])\n",
    "    axarr[row, 0].set_ylabel('{}'.format(classes[y[row]]), fontsize=14)\n",
    "    \n",
    "    # Explanations\n",
    "    m = np.abs(values[row]).max()\n",
    "    for col in range(num_classes):\n",
    "        axarr[row, col + 1].imshow(values[row, col], cmap='seismic', vmin=-m, vmax=m)\n",
    "        axarr[row, col + 1].set_xticks([])\n",
    "        axarr[row, col + 1].set_yticks([])\n",
    "        if col == y[row]:\n",
    "            axarr[row, col + 1].set_xlabel('{:.2f}'.format(pred[row, col]), fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            axarr[row, col + 1].set_xlabel('{:.2f}'.format(pred[row, col]), fontsize=12)\n",
    "        \n",
    "        # Class labels\n",
    "        if row == 0:\n",
    "            axarr[row, col + 1].set_title('{}'.format(classes[y[col]]), fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447c8bf-0ee8-4f11-868a-6d3e1e156b35",
   "metadata": {},
   "source": [
    "# Validating Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfa86519-30e0-4a58-a8e4-7dd9d13a1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import surrogate_validate as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cf40398-e3b5-4f99-820a-dbdf52134088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 256])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wc/763jgpdd6m5gljcl93zb8c940000gn/T/ipykernel_38158/4203483485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_all_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurrogate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKLDivLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_surr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/NYU/2022 Spring/Machine Learning/Project/ml_project/notebooks/surrogate_validate.py\u001b[0m in \u001b[0;36mvalidate_all_ones\u001b[0;34m(surrogate, loss_fn, val_data, num_classes, validation_batch_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import surrogate_validate as sg\n",
    "# from surrogate import validate_all_ones\n",
    "\n",
    "# Select one image from each class\n",
    "dset = val_set\n",
    "targets = np.array(dset.targets)\n",
    "num_classes = targets.max() + 1\n",
    "# inds_lists = [np.where(targets == cat)[0] for cat in range(num_classes)]\n",
    "# inds = [np.random.choice(cat_inds) for cat_inds in inds_lists]\n",
    "# x, y = zip(*[dset[ind] for ind in inds])\n",
    "# x = torch.stack(x)\n",
    "\n",
    "# Get explanations\n",
    "# values = fastshap.shap_values(x.to(device))\n",
    "\n",
    "# Get predictions\n",
    "\n",
    "\n",
    "sg.validate_all_ones(surrogate, loss_fn=KLDivLoss(), val_data=val_surr, num_classes=num_classes, validation_batch_size=256)\n",
    "    \n",
    "    \n",
    "# pred = surrogate(\n",
    "#     x.to(device),\n",
    "#     torch.ones(num_classes, surrogate.num_players, device=device)\n",
    "# ).softmax(dim=1).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51db6424-bb86-4352-8899-b007b6986b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def validate(model):\n",
    "    n = 0\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "\n",
    "    for x, y in tqdm(val_loader):\n",
    "        # Move to GPU.\n",
    "        n += len(x)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Get predictions.\n",
    "        pred = model(\n",
    "            x.to(device),\n",
    "            torch.ones(x.size(0), surrogate.num_players, device=device)\n",
    "            ).softmax(dim=1).cpu()\n",
    "\n",
    "        # Update loss.\n",
    "        loss = loss_fn(pred, y).item()\n",
    "        mean_loss += len(x) * (loss - mean_loss) / n\n",
    "\n",
    "        # Update accuracy.\n",
    "        acc = (torch.argmax(pred, dim=1) == y).float().mean().item()\n",
    "        mean_acc += len(x) * (acc - mean_acc) / n\n",
    "\n",
    "    return mean_loss, mean_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57fa48-ade7-42cd-9be6-b348179d47b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b43d46a73cd4d9dab797c4124693259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "with torch.no_grad():\n",
    "    validate(surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "563422de-4e0a-42d5-87d2-f2aff681130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4063f22-e453-4759-99f0-e2538df19592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 32])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a19624fd-93f1-4575-9e8d-f1cdbea591e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8cacf4c-e9d6-416c-9685-2e1708798edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image from each class\n",
    "dset = val_set\n",
    "targets = np.array(dset.targets)\n",
    "num_classes = targets.max() + 1\n",
    "inds_lists = [np.where(targets == cat)[0] for cat in range(num_classes)]\n",
    "inds = [np.random.choice(cat_inds) for cat_inds in inds_lists]\n",
    "x, y = zip(*[dset[ind] for ind in inds])\n",
    "x = torch.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13064892-f2d0-49dd-beff-7df96012208a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88a629e7-bf8e-4d73-be06-40680fceae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "pred = surrogate(\n",
    "    x.to(device),\n",
    "    torch.ones(x.size(0), surrogate.num_players, device=device)\n",
    ").softmax(dim=1).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11e474a0-4a3f-43d1-a2d6-711912e79755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a404f4-9d19-488e-bc10-6bfde4e92c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
