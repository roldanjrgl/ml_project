{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ec2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c73d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations, same as the ViT training scripts if without autoaug policy\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f54c46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load train set\n",
    "train_set = dsets.CIFAR10('../data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# Load test set (using as validation)\n",
    "val_set = dsets.CIFAR10('../data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec54b9",
   "metadata": {},
   "source": [
    "# train vit model on cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb842478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os.path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10684675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2204415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f11265",
   "metadata": {},
   "source": [
    "### training of the original model (ViT but lighter) is handled by the scripts in directory ViT-CIFAR\n",
    "\n",
    "we just need to load the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9cd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c314cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../trained_models/cifar_c10_vit_surrogate.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569b7f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(model_path):\n",
    "    # load saved model\n",
    "    print(\"Loading saved model\")\n",
    "    model = torch.load(model_path).to(device)\n",
    "else:\n",
    "    # Create model from trained\n",
    "    print(\"Creating model, from elsewhere pretrained\")\n",
    "    model = ViT(in_c = 3, num_classes=10, img_size=32, patch=8, dropout=0.0, num_layers=7,\n",
    "           hidden=384, mlp_hidden=384, head=12, is_cls_token=True)\n",
    "    loaded = torch.load(\"../ViT-CIFAR/weights/vit_c10_aa_ls_note_oriforFastSHAP.pth\")\n",
    "    model_states = {key[6:]:loaded[key] for key in loaded}\n",
    "    model.load_state_dict(model_states)\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc36e7",
   "metadata": {},
   "source": [
    "SAME as the resnet model, ViT model outputs unnormalized logits (scores) for each class, which serve as inputs to the Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad52525",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1e43f",
   "metadata": {},
   "source": [
    "# train surrogates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff9508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastshap import ImageSurrogate\n",
    "from fastshap.utils import MaskLayer2d, DatasetInputOnly, KLDivLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "108624d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "surr_model_path = '../trained_models/cifar_c10_vit_surrogate_lb20.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c2c6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved surrogate model\n"
     ]
    }
   ],
   "source": [
    "# Check for model\n",
    "if os.path.isfile(surr_model_path):\n",
    "    print('Loading saved surrogate model')\n",
    "    surr = torch.load(surr_model_path).to(device)\n",
    "    surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)\n",
    "else:\n",
    "    print(\"Creating new surrogate model\")\n",
    "    surr = nn.Sequential(\n",
    "        MaskLayer2d(value=0, append=True),\n",
    "        ViT(in_c = 4, num_classes=10, img_size=32, patch=8, dropout=0.0, num_layers=7,\n",
    "        hidden=384, mlp_hidden=384, head=12, is_cls_token=True)).to(device)\n",
    "    # Set up surrogate object\n",
    "    surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be070551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrain just go from here \n",
    "# print(\"Creating new surrogate model\")\n",
    "# surr = nn.Sequential(\n",
    "#     MaskLayer2d(value=0, append=True),\n",
    "#     ViT(in_c = 4, num_classes=10, img_size=32, patch=8, dropout=0.0, num_layers=7,\n",
    "#     hidden=384, mlp_hidden=384, head=12, is_cls_token=True)).to(device)\n",
    "# # Set up surrogate object\n",
    "# surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2488eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up datasets\n",
    "train_surr = DatasetInputOnly(train_set)\n",
    "val_surr = DatasetInputOnly(val_set)\n",
    "original_model = nn.Sequential(model, nn.Softmax(dim=1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d391ccc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wc/763jgpdd6m5gljcl93zb8c940000gn/T/ipykernel_33976/285035966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m surrogate.train_original_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_surr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_surr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moriginal_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/fastshap/image_surrogate.py\u001b[0m in \u001b[0;36mtrain_original_model\u001b[0;34m(self, train_data, val_data, original_model, batch_size, max_epochs, loss_fn, validation_samples, validation_batch_size, lr, min_lr, lr_factor, lookback, training_seed, validation_seed, num_workers, bar, verbose)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Generate validation labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             y_val = generate_labels(val_data, original_model,\n\u001b[0m\u001b[1;32m    330\u001b[0m                                     validation_batch_size, num_workers)\n\u001b[1;32m    331\u001b[0m             y_val_repeat = y_val.repeat(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/fastshap/image_surrogate.py\u001b[0m in \u001b[0;36mgenerate_labels\u001b[0;34m(dataset, model, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/fastshap/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tuple)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "surrogate.train_original_model(\n",
    "    train_surr,\n",
    "    val_surr,\n",
    "    original_model,\n",
    "    batch_size=256,\n",
    "    max_epochs=100,\n",
    "    loss_fn=KLDivLoss(),\n",
    "    lookback=20, #10\n",
    "    bar=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "surr.cpu()\n",
    "# torch.save(surr, 'cifar_c10_vit_surrogate_lb20.pt') # lookback 20, best val loss 0.7623\n",
    "torch.save(surr, surr_model_path) # lookback 20, best val loss, best vak kiss 0.5698\n",
    "# ls 0.827"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d77c45",
   "metadata": {},
   "source": [
    "# train fastshap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "171e3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "from fastshap import FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_path = 'cifar_explainer_unet_lb20.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model\n",
    "if os.path.isfile(explainer_path):\n",
    "    print('Loading saved explainer model')\n",
    "    explainer = torch.load(explainer_path).to(device)\n",
    "    fastshap = FastSHAP(explainer, surrogate, link=nn.LogSoftmax(dim=1))\n",
    "\n",
    "else:\n",
    "    # Set up explainer model\n",
    "    explainer = UNet(n_classes=10, num_down=2, num_up=1, num_convs=3).to(device)\n",
    "\n",
    "    # Set up FastSHAP object\n",
    "    fastshap = FastSHAP(explainer, surrogate, link=nn.LogSoftmax(dim=1))\n",
    "\n",
    "    # Set up datasets\n",
    "    fastshap_train = DatasetInputOnly(train_set)\n",
    "    fastshap_val = DatasetInputOnly(val_set)\n",
    "\n",
    "    # Train\n",
    "    fastshap.train(\n",
    "        fastshap_train,\n",
    "        fastshap_val,\n",
    "        batch_size=128,\n",
    "        num_samples=2,\n",
    "        max_epochs=200,\n",
    "        eff_lambda=1e-2,\n",
    "        validation_samples=1,\n",
    "        lookback=10,\n",
    "        bar=True,\n",
    "        verbose=True)\n",
    "    \n",
    "    # Save explainer\n",
    "    explainer.cpu()\n",
    "    torch.save(explainer, explainer_path)\n",
    "    explainer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395d911",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image from each class\n",
    "dset = val_set\n",
    "targets = np.array(dset.targets)\n",
    "num_classes = targets.max() + 1\n",
    "inds_lists = [np.where(targets == cat)[0] for cat in range(num_classes)]\n",
    "inds = [np.random.choice(cat_inds) for cat_inds in inds_lists]\n",
    "x, y = zip(*[dset[ind] for ind in inds])\n",
    "x = torch.stack(x)\n",
    "\n",
    "# Get explanations\n",
    "values = fastshap.shap_values(x.to(device))\n",
    "\n",
    "# Get predictions\n",
    "pred = surrogate(\n",
    "    x.to(device),\n",
    "    torch.ones(num_classes, surrogate.num_players, device=device)\n",
    ").softmax(dim=1).cpu().data.numpy()\n",
    "\n",
    "fig, axarr = plt.subplots(num_classes, num_classes + 1, figsize=(22, 20))\n",
    "\n",
    "for row in range(num_classes):\n",
    "    # Image\n",
    "    classes = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])[:, np.newaxis, np.newaxis]\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])[:, np.newaxis, np.newaxis]\n",
    "    im = x[row].numpy() * std + mean\n",
    "    im = im.transpose(1, 2, 0).astype(float)\n",
    "    im = np.clip(im, a_min=0, a_max=1)\n",
    "    axarr[row, 0].imshow(im, vmin=0, vmax=1)\n",
    "    axarr[row, 0].set_xticks([])\n",
    "    axarr[row, 0].set_yticks([])\n",
    "    axarr[row, 0].set_ylabel('{}'.format(classes[y[row]]), fontsize=14)\n",
    "    \n",
    "    # Explanations\n",
    "    m = np.abs(values[row]).max()\n",
    "    for col in range(num_classes):\n",
    "        axarr[row, col + 1].imshow(values[row, col], cmap='seismic', vmin=-m, vmax=m)\n",
    "        axarr[row, col + 1].set_xticks([])\n",
    "        axarr[row, col + 1].set_yticks([])\n",
    "        if col == y[row]:\n",
    "            axarr[row, col + 1].set_xlabel('{:.2f}'.format(pred[row, col]), fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            axarr[row, col + 1].set_xlabel('{:.2f}'.format(pred[row, col]), fontsize=12)\n",
    "        \n",
    "        # Class labels\n",
    "        if row == 0:\n",
    "            axarr[row, col + 1].set_title('{}'.format(classes[y[col]]), fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447c8bf-0ee8-4f11-868a-6d3e1e156b35",
   "metadata": {},
   "source": [
    "# Validating Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cf40398-e3b5-4f99-820a-dbdf52134088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 256])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wc/763jgpdd6m5gljcl93zb8c940000gn/T/ipykernel_33976/1244383558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_all_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurrogate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKLDivLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_surr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/NYU/2022 Spring/Machine Learning/Project/ml_project/notebooks/surrogate.py\u001b[0m in \u001b[0;36mvalidate_all_ones\u001b[0;34m(surrogate, loss_fn, val_data, num_classes, validation_batch_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import surrogate as sg\n",
    "# from surrogate import validate_all_ones\n",
    "\n",
    "# Select one image from each class\n",
    "dset = val_set\n",
    "targets = np.array(dset.targets)\n",
    "num_classes = targets.max() + 1\n",
    "# inds_lists = [np.where(targets == cat)[0] for cat in range(num_classes)]\n",
    "# inds = [np.random.choice(cat_inds) for cat_inds in inds_lists]\n",
    "# x, y = zip(*[dset[ind] for ind in inds])\n",
    "# x = torch.stack(x)\n",
    "\n",
    "# Get explanations\n",
    "# values = fastshap.shap_values(x.to(device))\n",
    "\n",
    "# Get predictions\n",
    "\n",
    "\n",
    "sg.validate_all_ones(surrogate, loss_fn=KLDivLoss(), val_data=val_surr, num_classes=num_classes, validation_batch_size=256)\n",
    "    \n",
    "    \n",
    "# pred = surrogate(\n",
    "#     x.to(device),\n",
    "#     torch.ones(num_classes, surrogate.num_players, device=device)\n",
    "# ).softmax(dim=1).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57fa48-ade7-42cd-9be6-b348179d47b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563422de-4e0a-42d5-87d2-f2aff681130c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
